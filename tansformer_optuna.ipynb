{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rezaie/Kire_mama/blob/master/tansformer_optuna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ccae883-1904-48a6-81b3-d0228caeab1f",
      "metadata": {
        "id": "9ccae883-1904-48a6-81b3-d0228caeab1f"
      },
      "source": [
        "### Multivariate time series prediction using transformer with hyperparameter optimization\n",
        "\n",
        "In this notebook, we use **Optuna** to find the optimum values of hyperparameters. In this notebook we specifically optimize the values of **learning rate, weight decay, positional encoding dropout** and **encoder layer dropout**.\n",
        "\n",
        "Optuna is a python package specifially designed for hyperparameter tuning. We need to define a range of possible values for each of the hyperparameters. And optuna will try different parameter values with the model to minimize the validation loss after for specified number of experiments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e04f754c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e04f754c",
        "outputId": "ded798c2-97ea-48f9-b1f0-faacaa88d475"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.42)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.4 colorlog-6.9.0 optuna-4.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4117e9d0-7925-4788-bbe1-28c60c80bb56",
      "metadata": {
        "id": "4117e9d0-7925-4788-bbe1-28c60c80bb56"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8af4f85b-3822-4288-9566-0f0ac0cfa053",
      "metadata": {
        "id": "8af4f85b-3822-4288-9566-0f0ac0cfa053"
      },
      "outputs": [],
      "source": [
        "path = 'final_data.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc9eb646-4308-449c-84c0-00cd1de182c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc9eb646-4308-449c-84c0-00cd1de182c2",
        "outputId": "a3a55176-b680-43c8-e549-eb06757fdfff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Implement determinism. Set a fixed value for random seed so that when the parameters are initialized, they are initialized same across all experiments.\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# If you are using CUDA, also set the seed for it\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Set the seed for NumPy\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0600e784-ec1c-4ab8-ab6b-9b765b4dee83",
      "metadata": {
        "id": "0600e784-ec1c-4ab8-ab6b-9b765b4dee83"
      },
      "source": [
        "Here we define **RiverData** a custom Dataset class to load the dataset we have. It extends the Pytorch Dataset class.  \n",
        "- We need to define \\_\\_init__() function which can be used for loading data from the file and optionally for data preprocessing.\n",
        "- Thereafter we define \\_\\_len__() function which gives the length of dataset.\n",
        "- Then we define \\_\\_getitem__() function which returns an instance of (feature, label) tuple which can be used for model training.\n",
        "  For our time series data, feature means the past values to be used for training and label means the future values to be predicted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "742b694d-9966-4fd3-bca7-51f398a8775f",
      "metadata": {
        "id": "742b694d-9966-4fd3-bca7-51f398a8775f"
      },
      "outputs": [],
      "source": [
        "class RiverData(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df, target, datecol, seq_len, pred_len):\n",
        "        self.df = df\n",
        "        self.datecol = datecol\n",
        "        self.target = target\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.setIndex()\n",
        "\n",
        "\n",
        "    def setIndex(self):\n",
        "        self.df.set_index(self.datecol, inplace=True)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df) - self.seq_len - self.pred_len\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if len(self.df) <= (idx + self.seq_len+self.pred_len):\n",
        "            raise IndexError(f\"Index {idx} is out of bounds for dataset of size {len(self.df)}\")\n",
        "        df_piece = self.df[idx:idx+self.seq_len].values\n",
        "        feature = torch.tensor(df_piece, dtype=torch.float32)\n",
        "        label_piece = self.df[self.target][idx + self.seq_len:  idx+self.seq_len+self.pred_len].values\n",
        "        label = torch.tensor(label_piece, dtype=torch.float32)\n",
        "        return (feature, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60c24959-e72f-435d-8a14-69a648fa5ab1",
      "metadata": {
        "id": "60c24959-e72f-435d-8a14-69a648fa5ab1"
      },
      "source": [
        "### Normalize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46d897d9-946e-4bbe-90a6-cc8d8a223c3f",
      "metadata": {
        "id": "46d897d9-946e-4bbe-90a6-cc8d8a223c3f"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(path)\n",
        "raw_df = df.drop('DATE', axis=1, inplace=False)\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply the transformations\n",
        "df_scaled = scaler.fit_transform(raw_df)\n",
        "\n",
        "df_scaled = pd.DataFrame(df_scaled, columns=raw_df.columns)\n",
        "df_scaled['DATE'] = df['DATE']\n",
        "df = df_scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a4781c6-88ed-4b28-aa0e-f669bf7c75f5",
      "metadata": {
        "id": "3a4781c6-88ed-4b28-aa0e-f669bf7c75f5"
      },
      "source": [
        "Some advanced Python syntax has been used here. \\\n",
        "*common_args : it's used to pass arguments to a function, where common_args represents a python list \\\n",
        "**common_args: it's used to pass arguments to a function, where common_args represents a python dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e63a535-a817-4429-be17-f3e3a02e4f73",
      "metadata": {
        "id": "9e63a535-a817-4429-be17-f3e3a02e4f73"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_size = int(0.7 * len(df))\n",
        "test_size = int(0.2 * len(df))\n",
        "val_size = len(df) - train_size - test_size\n",
        "\n",
        "seq_len = 13\n",
        "pred_len = 1\n",
        "num_features = 7\n",
        "num_layers = 1\n",
        "\n",
        "\n",
        "common_args = ['gauge_height', 'DATE', seq_len, pred_len]\n",
        "train_dataset = RiverData(df[:train_size], *common_args)\n",
        "val_dataset = RiverData(df[train_size: train_size+val_size], *common_args)\n",
        "test_dataset = RiverData(df[train_size+val_size : len(df)], *common_args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "501879b1-dd1c-4ff0-afb3-ff25a345df05",
      "metadata": {
        "id": "501879b1-dd1c-4ff0-afb3-ff25a345df05"
      },
      "outputs": [],
      "source": [
        "# Important parameters\n",
        "\n",
        "BATCH_SIZE = 128 # keep as big as can be handled by GPU and memory\n",
        "SHUFFLE = False # we don't shuffle the time series data\n",
        "DATA_LOAD_WORKERS = 1 # it depends on amount of data you need to load\n",
        "learning_rate = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae359aec-9435-4229-8587-5f120b0370b3",
      "metadata": {
        "id": "ae359aec-9435-4229-8587-5f120b0370b3"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "common_args = {'batch_size': BATCH_SIZE, 'shuffle': SHUFFLE}\n",
        "train_loader = DataLoader(train_dataset, **common_args)\n",
        "val_loader = DataLoader(val_dataset, **common_args)\n",
        "test_loader = DataLoader(test_dataset, **common_args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc7ac35c-8ecc-4ab7-a97c-0cb4d59bc624",
      "metadata": {
        "id": "bc7ac35c-8ecc-4ab7-a97c-0cb4d59bc624"
      },
      "source": [
        "### Here we define our PyTorch model.\n",
        "\n",
        "BasicTransformerNetwork is the model class, it extends the Module class provided by Pytorch. \\\n",
        "- We define \\_\\_init__() function. It sets up layers and defines the model parameters.\n",
        "- Also, we define forward() function which defines how the forwared pass computation occurs\n",
        "- We also implement PositionalEncoding class which is an important part of transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01cd8d0d-2058-4dfe-9046-78cde5fd2f58",
      "metadata": {
        "id": "01cd8d0d-2058-4dfe-9046-78cde5fd2f58"
      },
      "outputs": [],
      "source": [
        "# The transformer implementation in pytorch doesn't implement the\n",
        "# positional encoding which is an essential part of the transforemer model\n",
        "\n",
        "# Provide more description of positional encoding\n",
        "class PositionalEncoding(torch.nn.Module):\n",
        "    def __init__(self, d_model, pos_enc_dropout, max_len=5000):\n",
        "        super().__init__();\n",
        "        self.dropout = torch.nn.Dropout(p=pos_enc_dropout)\n",
        "\n",
        "        Xp = torch.zeros(max_len, d_model) # max_len x d_model\n",
        "        position = torch.arange(0, max_len).unsqueeze(1) # max_len x 1\n",
        "\n",
        "        # Generates an exponentially decreasing series of numbers\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)) #length: d_model/2\n",
        "\n",
        "        #Applying sine to even indices in the array; 2i\n",
        "        Xp[:, 0::2] = torch.sin(position.float() * div_term)\n",
        "\n",
        "        #Applying cosine to odd indices in the array; 2i + 1\n",
        "        Xp[:, 1::2] = torch.cos(position.float() * div_term)\n",
        "\n",
        "        Xp = Xp.unsqueeze(1)\n",
        "        self.register_buffer('Xp', Xp)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x  = x + self.Xp[:x.size(0)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "\n",
        "class BasicTransformerNetwork(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, seq_len, pred_len, enc_layer_dropout, pos_enc_dropout):\n",
        "        # call the constructor of the base class\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.num_features = num_features\n",
        "\n",
        "        # I don't think the embedding size should be this big. We will see.\n",
        "        self.embedding_size = 128 #The features are converted to 128 embeddings\n",
        "        self.num_layers = num_layers\n",
        "        self.pos_encoder = PositionalEncoding(self.embedding_size, pos_enc_dropout, 10000)\n",
        "\n",
        "        # dim_feedforward = 4 * d_model\n",
        "        # layer_norm_eps: A very small number (epsilon) added to the denominator during the Layer Normalization calculation.\n",
        "        self.encLayer = torch.nn.TransformerEncoderLayer(d_model=self.embedding_size, nhead=8,\n",
        "                                                 dim_feedforward=256, dropout=enc_layer_dropout, activation=\"relu\",\n",
        "                                                 layer_norm_eps=1e-05, batch_first=True)\n",
        "\n",
        "        self.transformerEnc = torch.nn.TransformerEncoder(self.encLayer, num_layers=self.num_layers)\n",
        "\n",
        "        self.input_fc = torch.nn.Linear(self.num_features, self.embedding_size)\n",
        "\n",
        "        self.prediction_head = torch.nn.Linear(self.embedding_size, self.pred_len)\n",
        "\n",
        "        # Create causal mask\n",
        "        self.register_buffer('causal_mask', self._generate_causal_mask(seq_len))\n",
        "\n",
        "\n",
        "    def _generate_causal_mask(self, seq_len):\n",
        "        \"\"\"\n",
        "        Generate causal mask for transformer encoder.\n",
        "        Returns upper triangular matrix with -inf in upper triangle (excluding diagonal)\n",
        "        \"\"\"\n",
        "        mask = torch.triu(torch.full((seq_len, seq_len), float('-inf')), diagonal=1)\n",
        "        return mask\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_fc(x) * np.sqrt(self.embedding_size)\n",
        "        x = self.pos_encoder(x)\n",
        "        out = self.transformerEnc(x, mask=self.causal_mask)\n",
        "        last_embedding = out[:, -1, :]\n",
        "        prediction = self.prediction_head(last_embedding)\n",
        "        prediction = prediction.squeeze(-1)\n",
        "        return prediction\n",
        "# Note that the gradients are stored inside the FC layer objects\n",
        "# For each training example we need to get rid of these gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f579dc18-584e-414b-ae8e-37f62e4b42f3",
      "metadata": {
        "id": "f579dc18-584e-414b-ae8e-37f62e4b42f3"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b600c877-3409-48f9-80e5-3fdde7731817",
      "metadata": {
        "id": "b600c877-3409-48f9-80e5-3fdde7731817",
        "outputId": "3139d2cb-c965-4b1e-f406-a626605d49cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.2.1+cu121\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5277794b-fe37-4595-8554-d26db5710e44",
      "metadata": {
        "id": "5277794b-fe37-4595-8554-d26db5710e44"
      },
      "outputs": [],
      "source": [
        "loss = torch.nn.MSELoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1afde643-9953-4129-8415-7b4836c31300",
      "metadata": {
        "id": "1afde643-9953-4129-8415-7b4836c31300"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b3cc7a8-9844-4eae-a601-4787513eb1cf",
      "metadata": {
        "id": "4b3cc7a8-9844-4eae-a601-4787513eb1cf",
        "outputId": "2d2bff7b-6aca-4d26-8db5-838b06d0ad14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "features shape:  torch.Size([512, 13, 7])\n",
            "labels shape:  torch.Size([512, 1])\n"
          ]
        }
      ],
      "source": [
        "for i, (f,l) in enumerate(train_loader):\n",
        "    print('features shape: ', f.shape)\n",
        "    print('labels shape: ', l.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22683e91-642a-494b-abc5-069dc6fa9eb6",
      "metadata": {
        "id": "22683e91-642a-494b-abc5-069dc6fa9eb6"
      },
      "outputs": [],
      "source": [
        "# define metrics\n",
        "import numpy as np\n",
        "epsilon = np.finfo(float).eps\n",
        "\n",
        "def wape_function(y, y_pred):\n",
        "    \"\"\"Weighted Average Percentage Error metric in the interval [0; 100]\"\"\"\n",
        "    y = np.array(y)\n",
        "    y_pred = np.array(y_pred)\n",
        "    nominator = np.sum(np.abs(np.subtract(y, y_pred)))\n",
        "    denominator = np.add(np.sum(np.abs(y)), epsilon)\n",
        "    wape = np.divide(nominator, denominator) * 100.0\n",
        "    return wape\n",
        "\n",
        "def nse_function(y, y_pred):\n",
        "    y = np.array(y)\n",
        "    y_pred = np.array(y_pred)\n",
        "    return (1-(np.sum((y_pred-y)**2)/np.sum((y-np.mean(y))**2)))\n",
        "\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    # following line prepares the model for evaulation mode. It disables dropout and batch normalization if they have\n",
        "    # are part of the model. For our simple model it's not necessary. Still I'm going to use it.\n",
        "\n",
        "    model.eval()\n",
        "    all_outputs = torch.empty(0, pred_len)\n",
        "    all_labels = torch.empty(0, pred_len)\n",
        "    for inputs, labels in data_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs).detach().cpu().unsqueeze(1)\n",
        "        all_outputs = torch.vstack((all_outputs, outputs))\n",
        "        all_labels = torch.vstack((all_labels, labels))\n",
        "\n",
        "    avg_val_loss = loss(all_outputs, all_labels)\n",
        "    nse = nse_function(all_labels.numpy(), all_outputs.numpy())\n",
        "    wape = wape_function(all_labels.numpy(), all_outputs.numpy())\n",
        "\n",
        "    print(f'NSE : {nse}', end=' ')\n",
        "    print(f'WAPE : {wape}', end=' ')\n",
        "    print(f'Validation Loss: {avg_val_loss}')\n",
        "    model.train()\n",
        "    return avg_val_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9beaf3a1-2c40-4fc6-b0f7-c352b5763231",
      "metadata": {
        "id": "9beaf3a1-2c40-4fc6-b0f7-c352b5763231",
        "outputId": "4aa1f444-19a4-4b99-c2da-c75e1d63a6da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-19 17:19:04,438] A new study created in memory with name: no-name-2213175a-c3b5-4b56-bf90-164e77b99d0f\n",
            "/local_scratch/slurm.1125506/ipykernel_3266291/2234128446.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
            "/local_scratch/slurm.1125506/ipykernel_3266291/2234128446.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n",
            "/local_scratch/slurm.1125506/ipykernel_3266291/2234128446.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  pos_enc_dropout = trial.suggest_uniform('pos_enc_dropout', 0.05, 0.3)\n",
            "/local_scratch/slurm.1125506/ipykernel_3266291/2234128446.py:6: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  enc_layer_dropout = trial.suggest_uniform('enc_layer_dropout', 0.1, 0.5)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Traning Loss: 0.02526580304560975 NSE : -2.6605618000030518 WAPE : 117.91830153052135 Validation Loss: 0.06249501183629036\n",
            "Epoch 2: Traning Loss: 0.01687834267906353 NSE : -0.08283412456512451 WAPE : 59.316866371845336 Validation Loss: 0.018486706539988518\n",
            "Epoch 3: Traning Loss: 0.017058147678889222 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-19 17:19:30,536] Trial 0 finished with value: 0.018486706539988518 and parameters: {'lr': 0.009710522346682626, 'weight_decay': 8.829487000121852e-05, 'pos_enc_dropout': 0.15459847164742846, 'enc_layer_dropout': 0.41149718189368745}. Best is trial 0 with value: 0.018486706539988518.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NSE : -0.22743380069732666 WAPE : 65.16451114665495 Validation Loss: 0.020955391228199005\n",
            "Early stopping!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/local_scratch/slurm.1125506/ipykernel_3266291/2234128446.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
            "/local_scratch/slurm.1125506/ipykernel_3266291/2234128446.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n",
            "/local_scratch/slurm.1125506/ipykernel_3266291/2234128446.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  pos_enc_dropout = trial.suggest_uniform('pos_enc_dropout', 0.05, 0.3)\n",
            "/local_scratch/slurm.1125506/ipykernel_3266291/2234128446.py:6: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  enc_layer_dropout = trial.suggest_uniform('enc_layer_dropout', 0.1, 0.5)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Traning Loss: 0.04963418806201027 NSE : -4.414577960968018 WAPE : 145.94704308856683 Validation Loss: 0.09244049340486526\n",
            "Epoch 2: Traning Loss: 0.016938189946074832 NSE : -0.0033941268920898438 WAPE : 51.86400010423687 Validation Loss: 0.017130468040704727\n",
            "Epoch 3: Traning Loss: 0.015962898387304192 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-19 17:19:55,149] Trial 1 finished with value: 0.017130468040704727 and parameters: {'lr': 0.00959028332535089, 'weight_decay': 0.00025349829631454116, 'pos_enc_dropout': 0.09234553759662469, 'enc_layer_dropout': 0.43978325259555706}. Best is trial 1 with value: 0.017130468040704727.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NSE : -0.4637782573699951 WAPE : 72.45283457396346 Validation Loss: 0.02499038726091385\n",
            "Early stopping!\n",
            "Epoch 1: Traning Loss: 0.026059553017697964 NSE : -1.1913225650787354 WAPE : 91.19737182209582 Validation Loss: 0.0374113954603672\n",
            "Epoch 2: Traning Loss: 0.014388464744614088 NSE : -0.5602973699569702 WAPE : 77.03471440152185 Validation Loss: 0.0266382098197937\n",
            "Epoch 3: Traning Loss: 0.009605657655585008 NSE : 0.5431891083717346 WAPE : 39.05955623262572 Validation Loss: 0.007798912934958935\n",
            "Epoch 4: Traning Loss: 0.005576367418489745 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-19 17:20:28,084] Trial 2 finished with value: 0.007798912934958935 and parameters: {'lr': 0.0004627292644511695, 'weight_decay': 0.0002285926689252311, 'pos_enc_dropout': 0.13278603860443067, 'enc_layer_dropout': 0.4602154056633878}. Best is trial 2 with value: 0.007798912934958935.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NSE : 0.426383376121521 WAPE : 42.19183476623824 Validation Loss: 0.009793080389499664\n",
            "Early stopping!\n",
            "Epoch 1: Traning Loss: 0.020971636429693796 NSE : -1.461517095565796 WAPE : 96.21684404543319 Validation Loss: 0.04202430322766304\n",
            "Epoch 2: Traning Loss: 0.01558543510645212 NSE : -0.7702598571777344 WAPE : 81.70125147639563 Validation Loss: 0.03022279590368271\n",
            "Epoch 3: Traning Loss: 0.010361221221838203 NSE : 0.49836844205856323 WAPE : 41.10559671034051 Validation Loss: 0.008564114570617676\n",
            "Epoch 4: Traning Loss: 0.004785359134486295 NSE : 0.7702738344669342 WAPE : 25.32768587794566 Validation Loss: 0.00392200518399477\n",
            "Epoch 5: Traning Loss: 0.0033664225646753226 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-19 17:21:09,205] Trial 3 finished with value: 0.00392200518399477 and parameters: {'lr': 0.0004011011231298408, 'weight_decay': 7.927339617381719e-05, 'pos_enc_dropout': 0.20359146672452283, 'enc_layer_dropout': 0.21567483436359192}. Best is trial 3 with value: 0.00392200518399477.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NSE : 0.7633612900972366 WAPE : 24.957746208559676 Validation Loss: 0.004040019121021032\n",
            "Early stopping!\n",
            "Epoch 1: Traning Loss: 0.019950345940982477 NSE : -2.238368272781372 WAPE : 110.75636531644697 Validation Loss: 0.055287111550569534\n",
            "Epoch 2: Traning Loss: 0.020843465119183092 NSE : -1.67460036277771 WAPE : 99.85345210702874 Validation Loss: 0.04566216468811035\n",
            "Epoch 3: Traning Loss: 0.017882670460979164 NSE : -0.20463204383850098 WAPE : 51.03282546027976 Validation Loss: 0.020566105842590332\n",
            "Epoch 4: Traning Loss: 0.014825216950706873 NSE : -0.10779905319213867 WAPE : 50.29839566769002 Validation Loss: 0.018912922590970993\n",
            "Epoch 5: Traning Loss: 0.015162449618470547 NSE : -0.10207593441009521 WAPE : 50.292637988848064 Validation Loss: 0.018815215677022934\n",
            "Epoch 6: Traning Loss: 0.01523772915494321 NSE : -0.09840452671051025 WAPE : 50.293122197313366 Validation Loss: 0.018752532079815865\n",
            "Epoch 7: Traning Loss: 0.015105192755174062 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-19 17:22:06,690] Trial 4 finished with value: 0.018752532079815865 and parameters: {'lr': 0.002467172895532683, 'weight_decay': 0.00010992903999061927, 'pos_enc_dropout': 0.29392054943457946, 'enc_layer_dropout': 0.2617260318544786}. Best is trial 3 with value: 0.00392200518399477.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NSE : -0.10489296913146973 WAPE : 50.29801710470806 Validation Loss: 0.018863309174776077\n",
            "Early stopping!\n",
            "Epoch 1: Traning Loss: 0.019332222694807052 NSE : -1.3092961311340332 WAPE : 92.42037675996572 Validation Loss: 0.039425499737262726\n",
            "Epoch 2: Traning Loss: 0.015417831471966353 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-19 17:22:23,131] Trial 5 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NSE : -0.7202922105789185 WAPE : 79.15980851403994 Validation Loss: 0.02936972863972187\n",
            "Epoch 1: Traning Loss: 0.018195879468363402 NSE : -2.0512161254882812 WAPE : 107.16848623404135 Validation Loss: 0.05209195241332054\n",
            "Epoch 2: Traning Loss: 0.017421404407111464 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-19 17:22:39,563] Trial 6 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NSE : -1.6650009155273438 WAPE : 99.71624503554266 Validation Loss: 0.045498281717300415\n",
            "Epoch 1: Traning Loss: 0.016603792007648513 NSE : -1.281212329864502 WAPE : 92.65081597049814 Validation Loss: 0.03894604369997978\n",
            "Epoch 2: Traning Loss: 0.01440567997618706 NSE : -0.2599024772644043 WAPE : 68.07679176500501 Validation Loss: 0.02150970697402954\n",
            "Epoch 3: Traning Loss: 0.008887054433521394 NSE : 0.7162939310073853 WAPE : 28.47475257827802 Validation Loss: 0.004843578208237886\n",
            "Epoch 4: Traning Loss: 0.0037551364429451073 NSE : 0.7737729549407959 WAPE : 23.766421710005968 Validation Loss: 0.0038622659631073475\n",
            "Epoch 5: Traning Loss: 0.0031668238027427855 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-19 17:23:20,665] Trial 7 finished with value: 0.0038622659631073475 and parameters: {'lr': 0.0005950865555327729, 'weight_decay': 0.0001651786938362955, 'pos_enc_dropout': 0.11007064752567254, 'enc_layer_dropout': 0.1947265400415874}. Best is trial 7 with value: 0.0038622659631073475.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NSE : 0.7404330968856812 WAPE : 25.383519515890136 Validation Loss: 0.004431460984051228\n",
            "Early stopping!\n",
            "Epoch 1: Traning Loss: 0.019630525303711444 NSE : -1.2078213691711426 WAPE : 91.89380445584476 Validation Loss: 0.0376930758357048\n",
            "Epoch 2: Traning Loss: 0.014951303791547865 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-19 17:23:37,046] Trial 8 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NSE : -1.0560908317565918 WAPE : 89.03857680039272 Validation Loss: 0.03510265424847603\n",
            "Epoch 1: Traning Loss: 0.038268693547866064 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-19 17:23:45,332] Trial 9 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NSE : -3.0509161949157715 WAPE : 127.9386435683382 Validation Loss: 0.06915935128927231\n",
            "Epoch 1: Traning Loss: 0.01977360012425762 NSE : -0.4256035089492798 WAPE : 72.73078784062746 Validation Loss: 0.024338649585843086\n",
            "Epoch 2: Traning Loss: 0.014318636849021472 NSE : -0.06326889991760254 WAPE : 62.67010243386064 Validation Loss: 0.01815267838537693\n",
            "Epoch 3: Traning Loss: 0.01100135541658111 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-19 17:24:09,935] Trial 10 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NSE : 0.34411829710006714 WAPE : 48.90324581661494 Validation Loss: 0.01119755394756794\n",
            "Epoch 1: Traning Loss: 0.028589519468812167 NSE : -0.6511213779449463 WAPE : 78.2417316562465 Validation Loss: 0.028188802301883698\n",
            "Epoch 2: Traning Loss: 0.01573601959812157 NSE : -0.18914413452148438 WAPE : 66.0924042306086 Validation Loss: 0.020301686599850655\n",
            "Epoch 3: Traning Loss: 0.012110012208778165 NSE : 0.19674944877624512 WAPE : 53.870956155011505 Validation Loss: 0.013713511638343334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-19 17:24:36,326] Trial 11 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Traning Loss: 0.019434022550532584 NSE : -1.3488309383392334 WAPE : 94.11339845320926 Validation Loss: 0.04010046273469925\n",
            "Epoch 2: Traning Loss: 0.014339354662768316 NSE : -0.1458975076675415 WAPE : 64.84525929979583 Validation Loss: 0.019563360139727592\n",
            "Epoch 3: Traning Loss: 0.009131759358451792 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-19 17:25:01,610] Trial 12 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NSE : 0.2692875862121582 WAPE : 48.76901882813804 Validation Loss: 0.012475104071199894\n",
            "Epoch 1: Traning Loss: 0.024047508215304787 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-19 17:25:09,817] Trial 13 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NSE : -1.8886244297027588 WAPE : 104.09810274797988 Validation Loss: 0.049316104501485825\n",
            "Epoch 1: Traning Loss: 0.01431838270743037 NSE : -1.3968610763549805 WAPE : 94.24099178571151 Validation Loss: 0.04092046245932579\n",
            "Epoch 2: Traning Loss: 0.016574616530947956 NSE : -0.046845436096191406 WAPE : 57.42133989462215 Validation Loss: 0.017872290685772896\n",
            "Epoch 3: Traning Loss: 0.01621873386181787 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-19 17:25:34,466] Trial 14 finished with value: 0.017872290685772896 and parameters: {'lr': 0.002370441266274233, 'weight_decay': 4.1407921189782365e-05, 'pos_enc_dropout': 0.2544278343992412, 'enc_layer_dropout': 0.3226205188745241}. Best is trial 7 with value: 0.0038622659631073475.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NSE : -0.08631289005279541 WAPE : 50.31096748020731 Validation Loss: 0.018546098843216896\n",
            "Early stopping!\n",
            "Epoch 1: Traning Loss: 0.0191222709759616 NSE : 0.17147666215896606 WAPE : 54.29679549076904 Validation Loss: 0.014144981279969215\n",
            "Epoch 2: Traning Loss: 0.011029868673883038 NSE : 0.17495042085647583 WAPE : 54.56627070739511 Validation Loss: 0.014085676521062851\n",
            "Epoch 3: Traning Loss: 0.00878416749218545 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-19 17:25:59,109] Trial 15 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NSE : 0.29518556594848633 WAPE : 50.09412132186446 Validation Loss: 0.012032958678901196\n",
            "Epoch 1: Traning Loss: 0.027422770276325305 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-19 17:26:07,308] Trial 16 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NSE : -1.6002135276794434 WAPE : 99.40964423531334 Validation Loss: 0.04439220204949379\n",
            "Epoch 1: Traning Loss: 0.018561938666469102 NSE : -0.11188352108001709 WAPE : 62.233975869163 Validation Loss: 0.018982650712132454\n",
            "Epoch 2: Traning Loss: 0.016781917063467357 NSE : 0.03336364030838013 WAPE : 57.55290373653988 Validation Loss: 0.01650291681289673\n",
            "Epoch 3: Traning Loss: 0.014454824327571591 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-19 17:26:31,926] Trial 17 pruned. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NSE : 0.11953943967819214 WAPE : 55.82810916418224 Validation Loss: 0.015031680464744568\n",
            "Epoch 1: Traning Loss: 0.024789026397077113 NSE : 0.05429583787918091 WAPE : 54.14845162458103 Validation Loss: 0.016145553439855576\n",
            "Epoch 2: Traning Loss: 0.018053230623889544 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-19 17:26:48,337] Trial 18 finished with value: 0.016145553439855576 and parameters: {'lr': 0.00022163253578322888, 'weight_decay': 0.0004269280238541515, 'pos_enc_dropout': 0.18892590096062278, 'enc_layer_dropout': 0.33171824077582357}. Best is trial 7 with value: 0.0038622659631073475.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NSE : -0.039455533027648926 WAPE : 49.55015713004888 Validation Loss: 0.017746126279234886\n",
            "Early stopping!\n",
            "Epoch 1: Traning Loss: 0.018440998612600498 NSE : -0.2747478485107422 WAPE : 51.922105120073134 Validation Loss: 0.021763157099485397\n",
            "Epoch 2: Traning Loss: 0.01293874411905175 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-19 17:27:04,721] Trial 19 finished with value: 0.021763157099485397 and parameters: {'lr': 0.0007367874301143976, 'weight_decay': 5.759965462648816e-05, 'pos_enc_dropout': 0.1237335792328222, 'enc_layer_dropout': 0.21557865621541583}. Best is trial 7 with value: 0.0038622659631073475.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NSE : -0.6221939325332642 WAPE : 78.81908422269223 Validation Loss: 0.02769494242966175\n",
            "Early stopping!\n",
            "Number of finished trials: 20\n",
            "Best trial:\n",
            "  Value (Best Validation Loss): 0.0038622659631073475\n",
            "  Params:\n",
            "    lr: 0.0005950865555327729\n",
            "    weight_decay: 0.0001651786938362955\n",
            "    pos_enc_dropout: 0.11007064752567254\n",
            "    enc_layer_dropout: 0.1947265400415874\n"
          ]
        }
      ],
      "source": [
        "from optuna.samplers import TPESampler\n",
        "def objective(trial):\n",
        "    # Here we define the search space of the hyper-parameters. Optuna uses byaesian optimization to find the optimal values of the hyperparameters.\n",
        "    learning_rate = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
        "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n",
        "    pos_enc_dropout = trial.suggest_uniform('pos_enc_dropout', 0.05, 0.3)\n",
        "    enc_layer_dropout = trial.suggest_uniform('enc_layer_dropout', 0.1, 0.5)\n",
        "\n",
        "\n",
        "    model = BasicTransformerNetwork(seq_len, pred_len, pos_enc_dropout, enc_layer_dropout)\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    num_epochs = 10\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 1\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = []\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs).unsqueeze(1)\n",
        "            loss_val = loss(outputs, labels)\n",
        "\n",
        "            # calculate gradients for back propagation\n",
        "            loss_val.backward()\n",
        "\n",
        "            # update the weights based on the gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            # reset the gradients, avoid gradient accumulation\n",
        "            optimizer.zero_grad()\n",
        "            epoch_loss.append(loss_val.item())\n",
        "\n",
        "        avg_train_loss = sum(epoch_loss)/len(epoch_loss)\n",
        "        print(f'Epoch {epoch+1}: Traning Loss: {avg_train_loss}', end=' ')\n",
        "        avg_val_loss = evaluate_model(model, val_loader)\n",
        "\n",
        "        # Check for improvement\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            epochs_no_improve = 0\n",
        "            # Save the best model\n",
        "            torch.save(model.state_dict(), 'best_model_trial.pth')\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve == patience:\n",
        "                print('Early stopping!')\n",
        "                # Load the best model before stopping\n",
        "                model.load_state_dict(torch.load('best_model_trial.pth'))\n",
        "                break\n",
        "\n",
        "        # Report intermediate objective value\n",
        "        trial.report(best_val_loss, epoch)\n",
        "\n",
        "        # Handle pruning based on the intermediate value\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return best_val_loss\n",
        "\n",
        "# Default sampler is TPESampler (Tree-structured Parzen Estimator).\n",
        "# This sampler is based on independent sampling and uses a Bayesian optimization approach to efficiently explore\n",
        "# the hyperparameter search space by building probability models of objective values.\n",
        "\n",
        "study = optuna.create_study(direction='minimize', sampler=TPESampler())\n",
        "\n",
        "# normally you run 100s of trials.\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "print('Number of finished trials:', len(study.trials))\n",
        "print('Best trial:')\n",
        "trial = study.best_trial\n",
        "\n",
        "print('  Value (Best Validation Loss):', trial.value)\n",
        "print('  Params:')\n",
        "for key, value in trial.params.items():\n",
        "    print(f'    {key}: {value}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7216e93-a35d-4aa5-83db-2e43ea380f6e",
      "metadata": {
        "id": "f7216e93-a35d-4aa5-83db-2e43ea380f6e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "722fee71-de14-463f-abf6-e35f9f002ae1",
      "metadata": {
        "id": "722fee71-de14-463f-abf6-e35f9f002ae1"
      },
      "outputs": [],
      "source": [
        "# Plot the results with the metrics inside it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a2f592f-d2be-4ea2-9e9e-d034d45f551d",
      "metadata": {
        "id": "9a2f592f-d2be-4ea2-9e9e-d034d45f551d"
      },
      "outputs": [],
      "source": [
        "import optuna.visualization as vis\n",
        "\n",
        "# Optimization history\n",
        "fig1 = vis.plot_optimization_history(study)\n",
        "fig1.write_html(\"optimization_history_transformer.html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c474aced-89cf-41c7-ae2c-b98c4b87b1b9",
      "metadata": {
        "id": "c474aced-89cf-41c7-ae2c-b98c4b87b1b9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}